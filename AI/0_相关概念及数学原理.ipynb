{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络学习笔记\n",
    "## 0-相关概念及数学原理\n",
    "> 2019-18-Mai 佚之狗 https://github.com/HookeLiu QQ: 616471607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习是一大类算法的统称, 这些算法能根据\"经验\"自动优化以求最优解. 神经网络算法作为机器学习的一种实现, 实质上也是一种优化算法, 它能自动提取输入数据的特征并进行分类.可以表示成`optimizer.minimize( Loss({x,y},θ) + Ω(θ) )`(其中Loss()表示损失函数, {x,y}表示数据集, θ表示权重参数, Ω()表示正则化)\n",
    "\n",
    "人工神经网络在**功能上**借鉴了生物神经网络, 即: 输入和权重相当于生物神经网相当于生物神经网络树突的连接强度; 对输入的处理(总和以及激活函数)就相当于生物神经细胞对刺激的动作.对于生物神经细胞来说这个输入通常是神经递质的类型和量,这些递质对细胞膜的刺激会使得膜电位变化并且会累加到轴突,当轴突上的膜电位达到某一程度时细胞体就会向后传递.生物细胞受刺激后发出的信号实际上是密度不同的脉冲,称为发放率,和PDM(脉冲密度调制)很相似.\n",
    "\n",
    "但实际上, 就目前来说, 人工神经网络都是在\"拟合一个函数\", 实际上就是把数学建模或者说特征提取的工作交给神经网络. 比如每两个神经元如果权重互为相反数, 调节权重和偏置就能调节一个“矩形波”, 以“极限”的思想, 无数个“矩形波”可以拟合出任何函数. 人工神经元只是模仿生物神经元的基本功能--信息输入→模式加工→动作输出,具体来说就是`数据*权重+偏置`作为输→传递函数作为细胞体处理强度→传输函数作为输出传递到轴突.\n",
    "一个人工神经元实质上的功能就是把输入数据×权重再求和之后加上偏置作为传递函数的输入, 即`out = activFuc( ∑(W.T*X+b) )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # 使用numpy库以便计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neron(w, x, b):\n",
    "    sum = np.dot(w,x) + b\n",
    "    out = relu(sum)\n",
    "    return {'out':out, 'inSum':sum}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络之所以能区分那么多的特征, 是因为网络的每一层都在处理不同层次的特征, 要想实现实现每个层次都能\"总结\"上一层的信息, 那就需要使用非线性的激活函数. 因为如果使用线性的话, 全连接的权重就没有层次关系了, 就相当于所有的层次都压缩到了一层, 神经网络处理特征的能力就会下降. 使用非线性激活函数就相当于让每个前一层的输出都逐层嵌套到后一层的激活函数中.\n",
    "常用的激活函数有:sigmoid/tanh/relu/elu等, 对于浅层神经网络来说使用什么激活函数的区别都不大. \n",
    "sigmoid是把任意实数压缩到[0,1]范围, 输入为0的时候输出是0.5, 所以通常可以表示\"概率\", 也就是能把任意实数映射到概率. 但此函数的导数最大值为1/4,这意味着很容易造成梯度消失; sigmoid是个非常经典的非线性激活函数, 历史悠久, 来源于最大熵和拉格朗日数乘法.\n",
    "tanh则是把任意实数压缩至[-1,1],相比sigmoid来说是0均匀分布,可以缓解梯度消失的问题;\n",
    "relu是把负数部分取0,正数部分为线性,所以没有梯度消失的问题,但relu的输出非负所以会出现梯度爆炸(即神经元不会再被激活).\n",
    "为了方便反向传播, 定义激活函数的时候最好顺便定义其导函数.\n",
    "本文示例的感知机是一种简单的二分类器, 相当于线性拟合(中学数学的ax+b=0), 就相当于\"尝试画出一条直线(称为决策边界)把两种数据分开\", 所以使用的激活函数是sign函数(out<0:-1,out = 0:0,out>=0:1), 因为感知机参数所绘制的分割直线上方的点大于0而下方的小于0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # f(x)=1/(1+e^-x), 求导:f'(x) = f(x) * (1 - f(x))\n",
    "    return 1 / ( 1 + np.exp(-x) )\n",
    "def deriv_sigmoid(x):\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "def tanh(x):                               \n",
    "    # f(x)=(e^x−e^−x)/(e^x+e^−x), 求导: f'(x) = 1 - f^2(x)                     \n",
    "    return ( np.exp( np.array(x) ) - np.exp( -1 * np.array(x) ) ) / ( np.exp( np.array(x) ) + np.exp( -1 * np.array(x) ) )\n",
    "def deriv_tanh(x):\n",
    "    fsx= tanh(x) * tanh(x)\n",
    "    return 1 - fsx\n",
    "def relu(x):\n",
    "    # f(x)=max(0,x), 求导: f'(x) = 1 if x>0 else 0\n",
    "    return x if x>0 else 0\n",
    "def deriv_relu(x):\n",
    "    return 1 if x>0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络可以看成一个\"黑盒\", 输入一组数据就会输出一组数据, 为了评价一个模型的\"好坏\"也就是与预期的偏离程度, 就需要定义一个`损失函数`. 定义损失函数并没有一个确切的要求, 通常是根据实际任务需求选择或制定一个合理的函数, 比如SVM(Support Vector Machine, 支持向量机)的损失函数是HL(Hinge Loss, 合页损失函数)`L[y(w.T*x+b)] = Abs[1-y(w.Txb)]`(常用形式是Li = ∑max(0, sj-syi+∆)); 最典型的CEE(Cross Entropy Error,交叉熵误差)`J(θ)=−1/m ∑(i=1,m)( yi*log⁡( hθ(xi) )+(1−yi)*log⁡( 1−hθ(xi) ) )`; 适用于线性回归的MSE(Mean Squared Error, 均方误差)`loss = 1/m ∑(i=1,m) ( (yi−yHeadi)^2 )`等. \n",
    "\n",
    "主旨就是定义一个函数用于衡量预测与期望值之间的差异, 并通过训练调整参数使得这个差异最小化(优化). 并且, 通过修正参数使损失函数最小化的常用方法是对各个参数做`梯度下降`. 也就是求出各个参数的梯度(可以简单当成\"参数对损失函数的影响程度及方向\", 通常做法是求损失函数对参数的偏导数), 然后各个参数沿着梯度相反的方向(也就是乘个-1)增加.\n",
    "\n",
    "所谓的\"模型好坏\"也可以理解成\"模型的熵\", 熵本身是表示\"不确定程度\". 对于机器学习来说, 一切信息都存在概率分布, 信息熵越大则信息量越大, 这个分布也就越难以确定.交叉熵是信息论的一个概念, 从字面意思上来说是指\"真实分布与非真实分布的交叉\", 描述两个概率分布情况的差异, 可以简单理解成\"算法使用非真实分布预测出的结果与真实分布情况的不确定性\", 数值越小则模型越接近期望. 一般的表达式为`H(p,q) = -∑(i=1,m) ( p(x)*log( q(x) ) )`(其中p代表期望值,q代表预测值)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_HL(lable, inSum): \n",
    "    # 感知机是一种二分类线性分类模型, 可以使用合页损失函数.Li = max(0, -lable*(ax+b)), 其中lable(ax+b)的绝对值代表样本到决策边界的距离 \n",
    "    # 当样本点(xi,yi)被正确分类时且yi(a*xi+b)大于1时损失为0; 否则就是偏离程度(1-yi(ax+b))\n",
    "    return np.max(0, -1 * lable * inSum)\n",
    "def loss_MSE(pred, lable):\n",
    "    # 均方误差通常是用于衡量两组数据之间的平均差异情况\n",
    "    return np.mean( (lable - pred) ** 2  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了\"训练网络\", 也就是根据输入和标签调整参数使得网络参数最接近最优解, 就需要通过优化算法对损失函数进行优化. 最常用的优化方法是`梯度下降法(Gradient Descent)`, 它有很多的变体(BGD, SGD, MBGD等). 一些情况下使用`牛顿法`也是不错的.\n",
    "\n",
    "其实**梯度相当于中学数学里`导数`概念的一个扩展**, 也就是说, 当只有一个参数(一元, 一个维度)的时候梯度的计算和导数的计算一致, 可以把梯度理解为求一个\"多维空间\"中某个参数在某点变化率最大的向量, 其大小为导数值(其实中学数学里的导数就是省略了方向的一元(一维空间)函数的梯度, 因为一元函数的梯度方向和自变量轴平行, 方向由正负号确定. 高中所学的导数可以近似视为\"一元偏导数\". 需要区分的是**梯度是矢量而某点的导数是个常量**). 它沿\"最陡峭\"的方向指向更大值, 而梯度下降就是让这个参数往梯度的反方向更新. 在一维曲线上过一点反应这个曲线变化率的是切线, 在高维空间中这样的直线则有很多条, 所以需要求每一个维度的`偏导数`以选择每一个维度的\"切线\". 在这里一个权重就是一个维度且一个偏置也是一个维度. 求梯度的操作和求偏导数的操作相同.\n",
    "\n",
    "偏导数, 字面意思上就是\"偏向于某个变量的导数\", 就是把非这个变量的部分视为常数进行计算.\n",
    "因为神经网络是逐层的, 所以损失函数总是可以对每个参数连续偏导的, 也就是说损失函数中某一个靠前的运算门可以根据其后一层的梯度来计算自身的梯度, 也叫链式求导, 这个过程叫反向传播. 用`链式求导法, z = f[u(x,y),v(x,y)]在(x,y)处的偏导数: ∂z/∂x = ∂f/∂u * ∂u/∂x + ∂f/∂v * ∂v/∂x ; ∂z/∂y = ∂f/∂u * ∂u/∂y + ∂f/∂v * ∂v/∂y`( 设u=u(x,y),v=v(xy),(x,y)处存在偏导数且z = f(u,v)在(u,v)处具有连续偏导数 )\n",
    "\n",
    "反向传播的操作可以简单理解成\"把正向传播的输出当做反向传播输入, 反向地依次传入每一个门的导函数, 最终求出损失函数对每一个参数的偏导数\"\n",
    "\n",
    "梯度下降表示为`θj =θj−α( ∂J(θ)/(∂θj) )`, 含义: 某个权重的更新是自身减去一个损失函数对权重偏导数的常数(`学习率`)倍, 即`某个权重的新值 = 旧值 - 学习速率*(损失函数对旧值的偏导数)`偏导数∂J(θ)/∂θj的定义是`1/m (i=1,m)〖(hθ(xi)−yi) xji 〗` 乘上一个常数项(0到1之间)的意义是在于控制更新的最大步幅, 防止更新参数的时候越过最优解而不能收敛(因为每一步迭代时参数的变化量难以确定, 如果正处于损失函数变化去势很大的点, 参数的变化也会很大, 过大的变化就无法到达最小值点). \n",
    "\n",
    "训练一个模型通常的套路是: \n",
    "> 1. 初始化权重和偏置(随机或者导入)\n",
    "> 2. 将数据前向传播\n",
    "> 3. 根据输出与预期的差异计算误差(损失)\n",
    "> > * 误差可接受→计算下一组数据\n",
    "> > * 误差过大→按梯度下降( θ-= LR*(∂J(θ)/(∂θj) )更新参数 (优化器)\n",
    "> 4. 判断上一次迭代过程中是否更新了参数\n",
    "> > * 更新了→继续迭代\n",
    "> > * 没有更新(说明所有预测值和期望值的误差都可接受)→跳出, 结束训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 假定一个数据集, 假设目的是训练一个二输入与门\n",
    "data = np.array([\n",
    "    [1,1],[1,0],\n",
    "    [0,1],[1,1],\n",
    "    [1,1],[0,0]\n",
    "])\n",
    "true = np.array([\n",
    "    [1],[0],\n",
    "    [0],[1],\n",
    "    [1],[0]\n",
    "    \n",
    "])\n",
    "# 把数据集可视化\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "for i in range( len(data) ):\n",
    "    if true[i] == 1: \n",
    "        plt.plot(data[i][0], data[i][1], 'ro')\n",
    "    else:\n",
    "        plt.plot(data[i][0], data[i][1], 'bo')\n",
    "plt.grid(True)\n",
    "# 根据肉眼观察, 这些点是线性可分的, 可以给出一个\"分界线\", 目测这条线可能会是-1.2x + 1.5 (当然，可能的组合是无数多的)\n",
    "x = np.arange(13) * 0.1\n",
    "line = -1.2 * x + 1.5\n",
    "plt.plot(x, line, marker=\"*\", linewidth=2, linestyle=\"--\", color=\"orange\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化了一个2输入感知机,初始参数为:w->[-0.05906863 -0.09742093],b->0\n"
     ]
    }
   ],
   "source": [
    "# 例：一个单细胞的2输入1输出感知机\n",
    "def init():\n",
    "    w = np.random.randn(2) * 0.1        # 每个权重初始值必须要有些差异，否则梯度都一样就没办法训练了\n",
    "    b = 0                               # 偏置对真个模型的影响不大, 初始值可以相对随意一些, 通常习惯取0\n",
    "    print( \"初始化了一个2输入感知机,初始参数为:w->%s,b->%s\" %(w,b) )\n",
    "    return [w,b]\n",
    "def feedforward(w, b, inp):\n",
    "    y = neron(w, inp, b)\n",
    "    return {'weight':w, 'bias':b, 'pred':np.sign( y.get('out') ), 'inSum':y.get('inSum'), 'input':inp}    # 返回正向传播的相关参数以便处理和调试\n",
    "def backpropagation(inp, pred, lable):  \n",
    "    '''\n",
    "    反向传播\n",
    "    # 如前文所述, 反向传播其实可以用链式求导法求损失函数对各个参数的偏导数\n",
    "    # 这里的一个节点可以拆分成3层门：3个乘法门(输入乘权重)→1个加法门(3个乘法门的结果与偏置相加)→1个tanh门(激活函数)→Pred[→1个HL门(损失函数)→Loss]\n",
    "    # 对于感知机来说, 误差只有0和1, 因为它只有对和不对两种情况, 不存在\"程度\"　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　\n",
    "    # 在这里用的是感知损失, L(w,b) = -∑yi(w*xi+b),但是感知机是不可微分的, 所以直接用∂J(θ)/∂θi = ∂(-1*yi*inSum)/∂θi = -yi*xi\n",
    "    # 优化的目的就是让损失函数最小化, 而损失函数的参数就是网络模型的参数(w和b).\n",
    "    # 根据链式求导法, 损失函数对每个参数的偏导数就相当于把分开计算的每一个门的梯度嵌套起来(本例用不上, 因为感知机只有一个节点, 可以直接求损失函数对参数的偏导)\n",
    "    '''\n",
    "    # ∂Loss/∂wi =-1*(lable-pred)*xi; ∂Loss/∂b = -1*lable\n",
    "    dLoss_dB = -1 * lable\n",
    "    dLoss_dW = -1 * lable * inp\n",
    "    #print(\"*---*debug:backpropagation:\",{'w':dLoss_dW, 'b':dLoss_dB})\n",
    "    return [dLoss_dW, dLoss_dB ]\n",
    "def update(w, b, dLoss_dW, dLoss_dB, LR):\n",
    "    w -= LR * dLoss_dW\n",
    "    b -= LR * dLoss_dB\n",
    "    print(\"*---*debug:update:\",[w,b])\n",
    "    return [w,b]\n",
    "    \n",
    "pcp = init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此已经实现了一个感知机的基本功能, 接下来需要对其训练并使用测试数据集评估."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, lable, epochs = 100, LR = 0.0616471607):\n",
    "    # LR, Learn Rate, 超参数_学习率, 选值通常很小, 根据程序员的经验和模型表现进行调试. 也常用随着LR随着epoch增加而减小的方法来改善收敛.\n",
    "    w, b = pcp[0], pcp[1]\n",
    "    predcs = np.zeros([np.shape(data)[0],1])                  # 记录模型对整个数据集的输出, 以便求总体平均误差\n",
    "    select = -1                                               # 记录分错数据的索引, 只针对分错的数据更新\n",
    "    count  = 0\n",
    "    for epoch in range(epochs):\n",
    "        for idx in range( len(data) ):                        # 依次对数据集中的每一个数据迭代一次, 总共迭代len(data)*epoch次\n",
    "            # 按照上述常规套路的4步中的后3步训练感知机\n",
    "            fw = feedforward(w, b, data[idx])\n",
    "            pred = fw.get('pred')\n",
    "            #print(\"*----*debug:idx:\",idx)\n",
    "            #print(\"*----*debug:predOrg:\",fw.get('pred'))\n",
    "            #print(\"*----*debug:lable:\",int(lable[idx]))\n",
    "            if pred != int(lable[idx]):                     # 选择一个误分点进行更新\n",
    "                select = idx\n",
    "                #print(\"*----*debug:select\",select, \"→\", pred,\" != \",int(lable[idx]))\n",
    "                bp = backpropagation(data[select], pred, lable[select])\n",
    "                ud = update( w, b, bp[0], bp[1], LR )\n",
    "                w, b = ud[0], ud[1]\n",
    "                count += 1\n",
    "            if select == -1:\n",
    "                break                                        # 不存在误分点就不需要训练\n",
    "        if epoch % 10 == 0:                                  # 每迭代10次输出当前状态并减小一点学习率\n",
    "            for ds in range( len( data ) ) :\n",
    "                fw = feedforward(w, b, data[ds])\n",
    "                print(\"*----*debug:fw:\",fw)\n",
    "                predcs[ds] = fw.get('pred')\n",
    "            print( \"迭代第%dCycle，Loss%.5f\" %( epoch+1, loss_MSE(predcs, lable) ) ) \n",
    "            if LR > 0.01:\n",
    "                LR -= 0.0002139223150\n",
    "    print( \"对%d个Sample(s)共更新了%d次，共迭代%depochs\" %( np.shape(data)[0], count, ( np.shape(data)[0] ) * (epoch+1) ) )\n",
    "    return [w,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[-0.05906863 -0.09742093]\n",
      "0\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06422569]), 'input': array([1, 0])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02587339]), 'input': array([0, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06164716]), 'input': array([0, 0])}\n",
      "迭代第1Cycle，Loss0.50000\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06422569]), 'input': array([1, 0])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02587339]), 'input': array([0, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06164716]), 'input': array([0, 0])}\n",
      "迭代第11Cycle，Loss0.50000\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06422569]), 'input': array([1, 0])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02587339]), 'input': array([0, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06164716]), 'input': array([0, 0])}\n",
      "迭代第21Cycle，Loss0.50000\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06422569]), 'input': array([1, 0])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02587339]), 'input': array([0, 1])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06164716]), 'input': array([0, 0])}\n",
      "迭代第31Cycle，Loss0.50000\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06422569]), 'input': array([1, 0])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02587339]), 'input': array([0, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06164716]), 'input': array([0, 0])}\n",
      "迭代第41Cycle，Loss0.50000\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06422569]), 'input': array([1, 0])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02587339]), 'input': array([0, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06164716]), 'input': array([0, 0])}\n",
      "迭代第51Cycle，Loss0.50000\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06422569]), 'input': array([1, 0])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02587339]), 'input': array([0, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06164716]), 'input': array([0, 0])}\n",
      "迭代第61Cycle，Loss0.50000\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06422569]), 'input': array([1, 0])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02587339]), 'input': array([0, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06164716]), 'input': array([0, 0])}\n",
      "迭代第71Cycle，Loss0.50000\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06422569]), 'input': array([1, 0])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02587339]), 'input': array([0, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06164716]), 'input': array([0, 0])}\n",
      "迭代第81Cycle，Loss0.50000\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06422569]), 'input': array([1, 0])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02587339]), 'input': array([0, 1])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.02845191]), 'input': array([1, 1])}\n",
      "*----*debug:fw: {'weight': array([ 0.00257853, -0.03577377]), 'bias': array([0.06164716]), 'pred': array([1.]), 'inSum': array([0.06164716]), 'input': array([0, 0])}\n",
      "迭代第91Cycle，Loss0.50000\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "*---*debug:update: [array([ 0.00257853, -0.03577377]), array([0.06164716])]\n",
      "对6个Sample(s)共更新了301次，共迭代600epochs\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[ 0.00257853 -0.03577377]\n",
      "[0.06164716]\n"
     ]
    }
   ],
   "source": [
    "w, b = pcp[0], pcp[1]\n",
    "\n",
    "predcs = np.zeros([np.shape(data)[0],1])\n",
    "for ds in range( len( data ) ) :\n",
    "                fw = feedforward(w, b, data[ds])\n",
    "                predcs[ds] = fw.get('pred')\n",
    "print(predcs)\n",
    "print(w)\n",
    "print(b)\n",
    "parameters = train(data, true)\n",
    "w, b = parameters[0], parameters[1]\n",
    "for ds in range( len( data ) ) :\n",
    "                fw = feedforward(w, b, data[ds])\n",
    "                predcs[ds] = fw.get('pred')\n",
    "print(predcs)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假定一个测试数据集\n",
    "test = np.array([\n",
    "    [1,0,1],[0,1,0],[1,0,1],[1,1,1],[1,1,1],[0,0,0],\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
